<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>polars.functions API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}.homelink{display:block;font-size:1.8em;font-weight:bold;padding-bottom:.5em;border-bottom:1px solid silver}.homelink img{max-width:30%;max-height:1.8em;margin:auto;margin-bottom:.1em;margin-right:.3em}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;margin-top:20px;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}*{font-size:100%;font-family:Ubuntu}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>polars.functions</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Union, TextIO, Optional, List, BinaryIO, Sequence, Any
from io import StringIO, BytesIO
import numpy as np
from pathlib import Path

try:
    import pandas as pd
except ImportError:
    pass

from .frame import DataFrame
from .series import Series
from .lazy import LazyFrame
import pyarrow as pa
import pyarrow.parquet
import pyarrow.csv
import pyarrow.compute
import builtins
import urllib.request
import io

from typing import Dict
from .datatypes import DataType
from urllib.parse import (  # noqa
    urlencode,
    urljoin,
    urlparse as parse_url,
    uses_netloc,
    uses_params,
    uses_relative,
)

_VALID_URLS = set(uses_relative + uses_netloc + uses_params)
_VALID_URLS.discard(&#34;&#34;)


def _process_http_file(path: str) -&gt; io.BytesIO:
    with urllib.request.urlopen(path) as f:
        return io.BytesIO(f.read())


def _is_url(url: str) -&gt; bool:
    &#34;&#34;&#34;Check to see if a URL has a valid protocol.

    Parameters
    ----------
    url
        str or unicode.

    Returns
    -------
    isurl : bool
        If `url` has a valid protocol return True otherwise False.
    &#34;&#34;&#34;
    try:
        return parse_url(url).scheme in _VALID_URLS
    except Exception:
        return False


def _prepare_file_arg(
    file: Union[str, TextIO, Path, BinaryIO]
) -&gt; Union[str, TextIO, Path, BinaryIO]:
    &#34;&#34;&#34;
    Utility for read_[csv, parquet]. (not to be used by scan_[csv, parquet]).

    Does one of:
        - A path.Path object is converted to a string.
        - A raw file on the web is downloaded into a buffer.
    &#34;&#34;&#34;
    if _is_url(file):
        if isinstance(file, Path):
            file = str(file)

        if isinstance(file, str) and file.startswith(&#34;http&#34;):
            file = _process_http_file(file)
    else:
        if isinstance(file, StringIO):
            return io.BytesIO(file.read().encode(&#34;utf8&#34;))
        elif isinstance(file, BytesIO):
            return file
    return file


def get_dummies(df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Convert categorical variables into dummy/indicator variables.

    Parameters
    ----------
    df
        DataFrame to convert.
    &#34;&#34;&#34;
    return df.to_dummies()


def read_csv(
    file: Union[str, TextIO, Path, BinaryIO],
    infer_schema_length: int = 100,
    batch_size: int = 8192,
    has_headers: bool = True,
    ignore_errors: bool = False,
    stop_after_n_rows: Optional[int] = None,
    skip_rows: int = 0,
    projection: Optional[List[int]] = None,
    sep: str = &#34;,&#34;,
    columns: Optional[List[str]] = None,
    rechunk: bool = True,
    encoding: str = &#34;utf8&#34;,
    n_threads: Optional[int] = None,
    dtype: &#34;Optional[Dict[str, DataType]]&#34; = None,
    new_columns: &#34;Optional[List[str]]&#34; = None,
    use_pyarrow: bool = True,
    low_memory: bool = False,
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Read into a DataFrame from a csv file.

    Parameters
    ----------
    file
        Path to a file or a file like object.
        By file-like object, we refer to objects with a ``read()`` method,
        such as a file handler (e.g. via builtin ``open`` function)
        or ``StringIO`` or ``BytesIO``.
    infer_schema_length
        Maximum number of lines to read to infer schema.
    batch_size
        Number of lines to read into the buffer at once. Modify this to change performance.
    has_headers
        Indicate if first row of dataset is header or not. If set to False first row will be set to `column_x`,
        `x` being an enumeration over every column in the dataset.
    ignore_errors
        Try to keep reading lines if some lines yield errors.
    stop_after_n_rows
        After n rows are read from the CSV, it stops reading.
        During multi-threaded parsing, an upper bound of `n` rows
        cannot be guaranteed.
    skip_rows
        Start reading after `skip_rows`.
    projection
        Indexes of columns to select. Note that column indexes count from zero.
    sep
        Delimiter/ value separator.
    columns
        Columns to project/ select.
    rechunk
        Make sure that all columns are contiguous in memory by aggregating the chunks into a single array.
    encoding
        - &#34;utf8&#34;
        - &#34;utf8-lossy&#34;
    n_threads
        Number of threads to use in csv parsing. Defaults to the number of physical cpu&#39;s of your system.
    dtype
        Overwrite the dtypes during inference.
    new_columns
        Rename columns to these right after parsing. Note that the length of this list must equal the width of the DataFrame
        that&#39;s parsed.
    use_pyarrow
        Try to use pyarrow&#39;s native CSV parser. This is not always possible. The set of arguments given to this function
        determine if it is possible to use pyarrows native parser. Note that pyarrow and polars may have a different
        strategy regarding type inference.
    low_memory
        Reduce memory usage in expense of performance.

    Returns
    -------
    DataFrame
    &#34;&#34;&#34;
    file = _prepare_file_arg(file)

    if columns and not has_headers:
        for column in columns:
            if not column.startswith(&#34;column_&#34;):
                raise ValueError(
                    &#39;Specified column names do not start with &#34;column_&#34;, &#39;
                    &#34;but autogenerated header names were requested.&#34;
                )

    if (
        use_pyarrow
        and dtype is None
        and stop_after_n_rows is None
        and n_threads is None
        and encoding == &#34;utf8&#34;
        and not low_memory
    ):
        include_columns = None

        if columns:
            if not has_headers:
                # Convert &#39;column_1&#39;, &#39;column_2&#39;, ... column names to &#39;f0&#39;, &#39;f1&#39;, ... column names for pyarrow,
                # if CSV file does not contain a header.
                include_columns = [f&#34;f{int(column[7:]) - 1}&#34; for column in columns]
            else:
                include_columns = columns

        if not columns and projection:
            # Convert column indices from projection to &#39;f0&#39;, &#39;f1&#39;, ... column names for pyarrow.
            include_columns = [f&#34;f{column_idx}&#34; for column_idx in projection]

        tbl = pa.csv.read_csv(
            file,
            pa.csv.ReadOptions(
                skip_rows=skip_rows, autogenerate_column_names=not has_headers
            ),
            pa.csv.ParseOptions(delimiter=sep),
            pa.csv.ConvertOptions(
                column_types=None,
                include_columns=include_columns,
                include_missing_columns=ignore_errors,
            ),
        )

        if new_columns:
            tbl = tbl.rename_columns(new_columns)
        elif not has_headers:
            # Rename &#39;f0&#39;, &#39;f1&#39;, ... columns names autogenated by pyarrow to &#39;column_1&#39;, &#39;column_2&#39;, ...
            tbl = tbl.rename_columns(
                [f&#34;column_{int(column[1:]) + 1}&#34; for column in tbl.column_names]
            )

        return from_arrow(tbl, rechunk)

    def read_csv_to_df(file):
        return DataFrame.read_csv(
            file=file,
            infer_schema_length=infer_schema_length,
            batch_size=batch_size,
            has_headers=has_headers,
            ignore_errors=ignore_errors,
            stop_after_n_rows=stop_after_n_rows,
            skip_rows=skip_rows,
            projection=projection,
            sep=sep,
            columns=columns,
            rechunk=rechunk,
            encoding=encoding,
            n_threads=n_threads,
            dtype=dtype,
            low_memory=low_memory,
        )

    if isinstance(file, str) and file.endswith(&#34;.gz&#34;):
        try:
            # Try to use python-isal for faster gzip decompresssion.
            from isal import igzip as gzip_mod
        except ImportError:
            # Use normal gzip module, if python-isal is not installed.
            import gzip as gzip_mod

        with gzip_mod.open(file, &#34;rb&#34;) as fh:
            df = read_csv_to_df(fh)
    else:
        df = read_csv_to_df(file)

    if new_columns:
        df.columns = new_columns
    return df


def scan_csv(
    file: Union[str, Path],
    has_headers: bool = True,
    ignore_errors: bool = False,
    sep: str = &#34;,&#34;,
    skip_rows: int = 0,
    stop_after_n_rows: &#34;Optional[int]&#34; = None,
    cache: bool = True,
    dtype: &#34;Optional[Dict[str, DataType]]&#34; = None,
    low_memory: bool = False,
) -&gt; &#34;LazyFrame&#34;:
    &#34;&#34;&#34;
    Lazily read from a csv file.

    This allows the query optimizer to push down predicates and projections to the scan level,
    thereby potentially reducing memory overhead.

    Parameters
    ----------
    file
        Path to a file.
    has_headers
        If the CSV file has headers or not.
    ignore_errors
        Try to keep reading lines if some lines yield errors.
    sep
        Delimiter/ value separator.
    skip_rows
        Start reading after `skip_rows`.
    stop_after_n_rows
        After n rows are read from the CSV, it stops reading.
        During multi-threaded parsing, an upper bound of `n` rows
        cannot be guaranteed.
    cache
        Cache the result after reading.
    dtype
        Overwrite the dtypes during inference.
    low_memory
        Reduce memory usage in expense of performance.
    &#34;&#34;&#34;
    if isinstance(file, Path):
        file = str(file)
    return LazyFrame.scan_csv(
        file=file,
        has_headers=has_headers,
        sep=sep,
        ignore_errors=ignore_errors,
        skip_rows=skip_rows,
        stop_after_n_rows=stop_after_n_rows,
        cache=cache,
        dtype=dtype,
        low_memory=low_memory,
    )


def scan_parquet(
    file: Union[str, Path],
    stop_after_n_rows: &#34;Optional[int]&#34; = None,
    cache: bool = True,
) -&gt; &#34;LazyFrame&#34;:
    &#34;&#34;&#34;
    Lazily read from a parquet file.

    This allows the query optimizer to push down predicates and projections to the scan level,
    thereby potentially reducing memory overhead.

    Parameters
    ----------
    file
        Path to a file.
    stop_after_n_rows
        After n rows are read from the parquet, it stops reading.
    cache
        Cache the result after reading.
    &#34;&#34;&#34;
    if isinstance(file, Path):
        file = str(file)
    return LazyFrame.scan_parquet(
        file=file, stop_after_n_rows=stop_after_n_rows, cache=cache
    )


def read_ipc(file: Union[str, BinaryIO, Path], use_pyarrow: bool = True) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Read into a DataFrame from Arrow IPC stream format. This is also called the feather format.

    Parameters
    ----------
    file
        Path to a file or a file like object.
    use_pyarrow
        Use pyarrow or rust arrow backend.

    Returns
    -------
    DataFrame
    &#34;&#34;&#34;
    file = _prepare_file_arg(file)
    return DataFrame.read_ipc(file, use_pyarrow)


def read_parquet(
    source: &#34;Union[str, BinaryIO, Path, List[str]]&#34;,
    stop_after_n_rows: &#34;Optional[int]&#34; = None,
    memory_map=True,
    columns: Optional[List[str]] = None,
    **kwargs,
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Read into a DataFrame from a parquet file.

    Parameters
    ----------
    source
        Path to a file | list of files, or a file like object. If the path is a directory, that directory will be used
        as partition aware scan.
    stop_after_n_rows
        After n rows are read from the parquet, it stops reading. Note: this cannot be used in partition aware parquet
        reads.
    memory_map
        Memory map underlying file. This will likely increase performance.
    columns
        Columns to project/ select.
    **kwargs
        kwargs for [pyarrow.parquet.read_table](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)

    Returns
    -------
    DataFrame
    &#34;&#34;&#34;
    source = _prepare_file_arg(source)
    if stop_after_n_rows is not None:
        return DataFrame.read_parquet(source, stop_after_n_rows=stop_after_n_rows)
    else:
        return from_arrow(
            pa.parquet.read_table(
                source, memory_map=memory_map, columns=columns, **kwargs
            )
        )


def arg_where(mask: &#34;Series&#34;):
    &#34;&#34;&#34;
    Get index values where Boolean mask evaluate True.

    Parameters
    ----------
    mask
        Boolean Series.

    Returns
    -------
    UInt32 Series
    &#34;&#34;&#34;
    return mask.arg_true()


def from_arrow_table(table: pa.Table, rechunk: bool = True) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    .. deprecated:: 7.3
        use `from_arrow`

    Create a DataFrame from an arrow Table.

    Parameters
    ----------
    a
        Arrow Table.
    rechunk
        Make sure that all data is contiguous.
    &#34;&#34;&#34;
    return DataFrame.from_arrow(table, rechunk)


def from_arrow(a: &#34;Union[pa.Table, pa.Array]&#34;, rechunk: bool = True) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Create a DataFrame from an arrow Table.

    Parameters
    ----------
    a
        Arrow Table.
    rechunk
        Make sure that all data is contiguous.
    &#34;&#34;&#34;
    if isinstance(a, pa.Table):
        return DataFrame.from_arrow(a, rechunk)
    if isinstance(a, pa.Array):
        return Series.from_arrow(&#34;&#34;, a)
    raise ValueError(f&#34;expected arrow table / array, got {a}&#34;)


def _from_pandas_helper(a: &#34;pandas.Series&#34;) -&gt; &#34;pa.Array&#34;:  # noqa: F821
    dtype = a.dtype
    if dtype == &#34;datetime64[ns]&#34;:
        # We first cast to ms because that&#39;s the unit of Date64,
        # Then we cast to via int64 to date64. Casting directly to Date64 lead to
        # loss of time information https://github.com/ritchie46/polars/issues/476
        arr = pa.array(np.array(a.values, dtype=&#34;datetime64[ms]&#34;))
        arr = pa.compute.cast(arr, pa.int64())
        return pa.compute.cast(arr, pa.date64())
    elif dtype == &#34;object&#34; and isinstance(a.iloc[0], str):
        return pa.array(a, pa.large_utf8())
    else:
        return pa.array(a)


def from_pandas(
    df: &#34;[pandas.DataFrame, pandas.DateTimeIndex]&#34;, rechunk: bool = True  # noqa: F821
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Convert from a pandas DataFrame to a polars DataFrame.

    Parameters
    ----------
    df
        DataFrame to convert.
    rechunk
        Make sure that all data is contiguous.

    Returns
    -------
    A Polars DataFrame
    &#34;&#34;&#34;
    if type(df) == pd.Series or type(df) == pd.DatetimeIndex:
        return from_arrow(_from_pandas_helper(df))

    # Note: we first tried to infer the schema via pyarrow and then modify the schema if needed.
    # However arrow 3.0 determines the type of a string like this:
    #       pa.array(array).type
    # needlessly allocating and failing when the string is too large for the string dtype.
    data = {}

    for name in df.columns:
        s = df[name]
        data[name] = _from_pandas_helper(s)

    table = pa.table(data)
    return from_arrow(table, rechunk)


def concat(dfs: &#34;List[DataFrame]&#34;, rechunk=True) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Aggregate all the Dataframes in a List of DataFrames to a single DataFrame.

    Parameters
    ----------
    dfs
        DataFrames to concatenate.
    rechunk
        rechunk the final DataFrame.
    &#34;&#34;&#34;
    assert len(dfs) &gt; 0
    df = dfs[0]
    for i in builtins.range(1, len(dfs)):
        try:
            df = df.vstack(dfs[i], in_place=False)
        # could have a double borrow (one mutable one ref)
        except RuntimeError:
            df.vstack(dfs[i].clone(), in_place=True)

    if rechunk:
        return df.rechunk()
    return df


def repeat(
    val: &#34;Union[int, float, str]&#34;, n: int, name: Optional[str] = None
) -&gt; &#34;Series&#34;:
    &#34;&#34;&#34;
    Repeat a single value n times and collect into a Series.

    Parameters
    ----------
    val
        Value to repeat.
    n
        Number of repeats.
    name
        Optional name of the Series.
    &#34;&#34;&#34;
    if name is None:
        name = &#34;&#34;
    if isinstance(val, str):
        s = Series._repeat(name, val, n)
        s.rename(name)
        return s
    else:
        return Series.from_arrow(name, pa.repeat(val, n))


def read_json(
    source: &#34;Union[str, StringIO, Path]&#34;,
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Read into a DataFrame from JSON format.

    Parameters
    ----------
    source
        Path to a file or a file like object.
    &#34;&#34;&#34;
    return DataFrame.read_json(source)


def from_rows(
    rows: &#34;Sequence[Sequence[Any]]&#34;,
    column_names: &#34;Optional[List[str]]&#34; = None,
    column_name_mapping: &#34;Optional[Dict[int, str]]&#34; = None,
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Create a DataFrame from rows. This should only be used as a last resort, as this is more expensive than
    creating from columnar data.

    Parameters
    ----------
    rows
        rows.
    column_names
        column names to use for the DataFrame.
    column_name_mapping
        map column index to a new name:
        Example:
        ```python
            column_mapping: {0: &#34;first_column, 3: &#34;fourth column&#34;}
        ```
    &#34;&#34;&#34;
    return DataFrame.from_rows(rows, column_names, column_name_mapping)


def read_sql(sql: str, engine) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    # Preface
    Deprecated by design. Will not have a long future support and no guarantees given whatsoever.
    Want backwards compatibility?

    Use:

    ```python
    df = pl.from_pandas(pd.read_sql(sql, engine))
    ```

    The support is limited because I want something better.

    # Docstring
    Load a DataFrame from a database by sending a raw sql query.
    Make sure to install sqlalchemy ^1.4

    Parameters
    ----------
    sql
        raw sql query
    engine : sqlalchemy engine
        make sure to install sqlalchemy ^1.4
    &#34;&#34;&#34;
    try:
        # pandas sql loading is faster.
        # conversion from pandas to arrow is very cheap compared to db driver
        import pandas as pd

        return from_pandas(pd.read_sql(sql, engine))
    except ImportError:
        from sqlalchemy import text

        with engine.connect() as con:
            result = con.execute(text(sql))

        rows = result.fetchall()
        return from_rows(rows, list(result.keys()))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="polars.functions.arg_where"><code class="name flex">
<span>def <span class="ident">arg_where</span></span>(<span>mask: Series)</span>
</code></dt>
<dd>
<div class="desc"><p>Get index values where Boolean mask evaluate True.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mask</code></strong></dt>
<dd>Boolean Series.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>UInt32 Series</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def arg_where(mask: &#34;Series&#34;):
    &#34;&#34;&#34;
    Get index values where Boolean mask evaluate True.

    Parameters
    ----------
    mask
        Boolean Series.

    Returns
    -------
    UInt32 Series
    &#34;&#34;&#34;
    return mask.arg_true()</code></pre>
</details>
</dd>
<dt id="polars.functions.concat"><code class="name flex">
<span>def <span class="ident">concat</span></span>(<span>dfs: List[DataFrame], rechunk=True) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Aggregate all the Dataframes in a List of DataFrames to a single DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dfs</code></strong></dt>
<dd>DataFrames to concatenate.</dd>
<dt><strong><code>rechunk</code></strong></dt>
<dd>rechunk the final DataFrame.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concat(dfs: &#34;List[DataFrame]&#34;, rechunk=True) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Aggregate all the Dataframes in a List of DataFrames to a single DataFrame.

    Parameters
    ----------
    dfs
        DataFrames to concatenate.
    rechunk
        rechunk the final DataFrame.
    &#34;&#34;&#34;
    assert len(dfs) &gt; 0
    df = dfs[0]
    for i in builtins.range(1, len(dfs)):
        try:
            df = df.vstack(dfs[i], in_place=False)
        # could have a double borrow (one mutable one ref)
        except RuntimeError:
            df.vstack(dfs[i].clone(), in_place=True)

    if rechunk:
        return df.rechunk()
    return df</code></pre>
</details>
</dd>
<dt id="polars.functions.from_arrow"><code class="name flex">
<span>def <span class="ident">from_arrow</span></span>(<span>a: Union[pa.Table, pa.Array], rechunk: bool = True) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create a DataFrame from an arrow Table.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Arrow Table.</dd>
<dt><strong><code>rechunk</code></strong></dt>
<dd>Make sure that all data is contiguous.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_arrow(a: &#34;Union[pa.Table, pa.Array]&#34;, rechunk: bool = True) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Create a DataFrame from an arrow Table.

    Parameters
    ----------
    a
        Arrow Table.
    rechunk
        Make sure that all data is contiguous.
    &#34;&#34;&#34;
    if isinstance(a, pa.Table):
        return DataFrame.from_arrow(a, rechunk)
    if isinstance(a, pa.Array):
        return Series.from_arrow(&#34;&#34;, a)
    raise ValueError(f&#34;expected arrow table / array, got {a}&#34;)</code></pre>
</details>
</dd>
<dt id="polars.functions.from_arrow_table"><code class="name flex">
<span>def <span class="ident">from_arrow_table</span></span>(<span>table: pyarrow.lib.Table, rechunk: bool = True) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><div class="admonition deprecated">
<p class="admonition-title">Deprecated since version:&ensp;7.3</p>
<p>use <code><a title="polars.functions.from_arrow" href="#polars.functions.from_arrow">from_arrow()</a></code></p>
</div>
<p>Create a DataFrame from an arrow Table.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Arrow Table.</dd>
<dt><strong><code>rechunk</code></strong></dt>
<dd>Make sure that all data is contiguous.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_arrow_table(table: pa.Table, rechunk: bool = True) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    .. deprecated:: 7.3
        use `from_arrow`

    Create a DataFrame from an arrow Table.

    Parameters
    ----------
    a
        Arrow Table.
    rechunk
        Make sure that all data is contiguous.
    &#34;&#34;&#34;
    return DataFrame.from_arrow(table, rechunk)</code></pre>
</details>
</dd>
<dt id="polars.functions.from_pandas"><code class="name flex">
<span>def <span class="ident">from_pandas</span></span>(<span>df: [pandas.DataFrame, pandas.DateTimeIndex], rechunk: bool = True) ‑> DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Convert from a pandas DataFrame to a polars DataFrame.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>DataFrame to convert.</dd>
<dt><strong><code>rechunk</code></strong></dt>
<dd>Make sure that all data is contiguous.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>A Polars DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_pandas(
    df: &#34;[pandas.DataFrame, pandas.DateTimeIndex]&#34;, rechunk: bool = True  # noqa: F821
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Convert from a pandas DataFrame to a polars DataFrame.

    Parameters
    ----------
    df
        DataFrame to convert.
    rechunk
        Make sure that all data is contiguous.

    Returns
    -------
    A Polars DataFrame
    &#34;&#34;&#34;
    if type(df) == pd.Series or type(df) == pd.DatetimeIndex:
        return from_arrow(_from_pandas_helper(df))

    # Note: we first tried to infer the schema via pyarrow and then modify the schema if needed.
    # However arrow 3.0 determines the type of a string like this:
    #       pa.array(array).type
    # needlessly allocating and failing when the string is too large for the string dtype.
    data = {}

    for name in df.columns:
        s = df[name]
        data[name] = _from_pandas_helper(s)

    table = pa.table(data)
    return from_arrow(table, rechunk)</code></pre>
</details>
</dd>
<dt id="polars.functions.from_rows"><code class="name flex">
<span>def <span class="ident">from_rows</span></span>(<span>rows: Sequence[Sequence[Any]], column_names: Optional[List[str]] = None, column_name_mapping: Optional[Dict[int, str]] = None) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create a DataFrame from rows. This should only be used as a last resort, as this is more expensive than
creating from columnar data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>rows</code></strong></dt>
<dd>rows.</dd>
<dt><strong><code>column_names</code></strong></dt>
<dd>column names to use for the DataFrame.</dd>
<dt><strong><code>column_name_mapping</code></strong></dt>
<dd>map column index to a new name:
Example:
<code>python
column_mapping: {0: "first_column, 3: "fourth column"}</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_rows(
    rows: &#34;Sequence[Sequence[Any]]&#34;,
    column_names: &#34;Optional[List[str]]&#34; = None,
    column_name_mapping: &#34;Optional[Dict[int, str]]&#34; = None,
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Create a DataFrame from rows. This should only be used as a last resort, as this is more expensive than
    creating from columnar data.

    Parameters
    ----------
    rows
        rows.
    column_names
        column names to use for the DataFrame.
    column_name_mapping
        map column index to a new name:
        Example:
        ```python
            column_mapping: {0: &#34;first_column, 3: &#34;fourth column&#34;}
        ```
    &#34;&#34;&#34;
    return DataFrame.from_rows(rows, column_names, column_name_mapping)</code></pre>
</details>
</dd>
<dt id="polars.functions.get_dummies"><code class="name flex">
<span>def <span class="ident">get_dummies</span></span>(<span>df: <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a>) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Convert categorical variables into dummy/indicator variables.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>DataFrame to convert.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dummies(df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Convert categorical variables into dummy/indicator variables.

    Parameters
    ----------
    df
        DataFrame to convert.
    &#34;&#34;&#34;
    return df.to_dummies()</code></pre>
</details>
</dd>
<dt id="polars.functions.read_csv"><code class="name flex">
<span>def <span class="ident">read_csv</span></span>(<span>file: Union[str, TextIO, pathlib.Path, BinaryIO], infer_schema_length: int = 100, batch_size: int = 8192, has_headers: bool = True, ignore_errors: bool = False, stop_after_n_rows: Union[int, NoneType] = None, skip_rows: int = 0, projection: Union[List[int], NoneType] = None, sep: str = ',', columns: Union[List[str], NoneType] = None, rechunk: bool = True, encoding: str = 'utf8', n_threads: Union[int, NoneType] = None, dtype: Optional[Dict[str, DataType]] = None, new_columns: Optional[List[str]] = None, use_pyarrow: bool = True, low_memory: bool = False) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Read into a DataFrame from a csv file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>Path to a file or a file like object.
By file-like object, we refer to objects with a <code>read()</code> method,
such as a file handler (e.g. via builtin <code>open</code> function)
or <code>StringIO</code> or <code>BytesIO</code>.</dd>
<dt><strong><code>infer_schema_length</code></strong></dt>
<dd>Maximum number of lines to read to infer schema.</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>Number of lines to read into the buffer at once. Modify this to change performance.</dd>
<dt><strong><code>has_headers</code></strong></dt>
<dd>Indicate if first row of dataset is header or not. If set to False first row will be set to <code>column_x</code>,
<code>x</code> being an enumeration over every column in the dataset.</dd>
<dt><strong><code>ignore_errors</code></strong></dt>
<dd>Try to keep reading lines if some lines yield errors.</dd>
<dt><strong><code>stop_after_n_rows</code></strong></dt>
<dd>After n rows are read from the CSV, it stops reading.
During multi-threaded parsing, an upper bound of <code>n</code> rows
cannot be guaranteed.</dd>
<dt><strong><code>skip_rows</code></strong></dt>
<dd>Start reading after <code>skip_rows</code>.</dd>
<dt><strong><code>projection</code></strong></dt>
<dd>Indexes of columns to select. Note that column indexes count from zero.</dd>
<dt><strong><code>sep</code></strong></dt>
<dd>Delimiter/ value separator.</dd>
<dt><strong><code>columns</code></strong></dt>
<dd>Columns to project/ select.</dd>
<dt><strong><code>rechunk</code></strong></dt>
<dd>Make sure that all columns are contiguous in memory by aggregating the chunks into a single array.</dd>
<dt><strong><code>encoding</code></strong></dt>
<dd>
<ul>
<li>"utf8"</li>
<li>"utf8-lossy"</li>
</ul>
</dd>
<dt><strong><code>n_threads</code></strong></dt>
<dd>Number of threads to use in csv parsing. Defaults to the number of physical cpu's of your system.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Overwrite the dtypes during inference.</dd>
<dt><strong><code>new_columns</code></strong></dt>
<dd>Rename columns to these right after parsing. Note that the length of this list must equal the width of the DataFrame
that's parsed.</dd>
<dt><strong><code>use_pyarrow</code></strong></dt>
<dd>Try to use pyarrow's native CSV parser. This is not always possible. The set of arguments given to this function
determine if it is possible to use pyarrows native parser. Note that pyarrow and polars may have a different
strategy regarding type inference.</dd>
<dt><strong><code>low_memory</code></strong></dt>
<dd>Reduce memory usage in expense of performance.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_csv(
    file: Union[str, TextIO, Path, BinaryIO],
    infer_schema_length: int = 100,
    batch_size: int = 8192,
    has_headers: bool = True,
    ignore_errors: bool = False,
    stop_after_n_rows: Optional[int] = None,
    skip_rows: int = 0,
    projection: Optional[List[int]] = None,
    sep: str = &#34;,&#34;,
    columns: Optional[List[str]] = None,
    rechunk: bool = True,
    encoding: str = &#34;utf8&#34;,
    n_threads: Optional[int] = None,
    dtype: &#34;Optional[Dict[str, DataType]]&#34; = None,
    new_columns: &#34;Optional[List[str]]&#34; = None,
    use_pyarrow: bool = True,
    low_memory: bool = False,
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Read into a DataFrame from a csv file.

    Parameters
    ----------
    file
        Path to a file or a file like object.
        By file-like object, we refer to objects with a ``read()`` method,
        such as a file handler (e.g. via builtin ``open`` function)
        or ``StringIO`` or ``BytesIO``.
    infer_schema_length
        Maximum number of lines to read to infer schema.
    batch_size
        Number of lines to read into the buffer at once. Modify this to change performance.
    has_headers
        Indicate if first row of dataset is header or not. If set to False first row will be set to `column_x`,
        `x` being an enumeration over every column in the dataset.
    ignore_errors
        Try to keep reading lines if some lines yield errors.
    stop_after_n_rows
        After n rows are read from the CSV, it stops reading.
        During multi-threaded parsing, an upper bound of `n` rows
        cannot be guaranteed.
    skip_rows
        Start reading after `skip_rows`.
    projection
        Indexes of columns to select. Note that column indexes count from zero.
    sep
        Delimiter/ value separator.
    columns
        Columns to project/ select.
    rechunk
        Make sure that all columns are contiguous in memory by aggregating the chunks into a single array.
    encoding
        - &#34;utf8&#34;
        - &#34;utf8-lossy&#34;
    n_threads
        Number of threads to use in csv parsing. Defaults to the number of physical cpu&#39;s of your system.
    dtype
        Overwrite the dtypes during inference.
    new_columns
        Rename columns to these right after parsing. Note that the length of this list must equal the width of the DataFrame
        that&#39;s parsed.
    use_pyarrow
        Try to use pyarrow&#39;s native CSV parser. This is not always possible. The set of arguments given to this function
        determine if it is possible to use pyarrows native parser. Note that pyarrow and polars may have a different
        strategy regarding type inference.
    low_memory
        Reduce memory usage in expense of performance.

    Returns
    -------
    DataFrame
    &#34;&#34;&#34;
    file = _prepare_file_arg(file)

    if columns and not has_headers:
        for column in columns:
            if not column.startswith(&#34;column_&#34;):
                raise ValueError(
                    &#39;Specified column names do not start with &#34;column_&#34;, &#39;
                    &#34;but autogenerated header names were requested.&#34;
                )

    if (
        use_pyarrow
        and dtype is None
        and stop_after_n_rows is None
        and n_threads is None
        and encoding == &#34;utf8&#34;
        and not low_memory
    ):
        include_columns = None

        if columns:
            if not has_headers:
                # Convert &#39;column_1&#39;, &#39;column_2&#39;, ... column names to &#39;f0&#39;, &#39;f1&#39;, ... column names for pyarrow,
                # if CSV file does not contain a header.
                include_columns = [f&#34;f{int(column[7:]) - 1}&#34; for column in columns]
            else:
                include_columns = columns

        if not columns and projection:
            # Convert column indices from projection to &#39;f0&#39;, &#39;f1&#39;, ... column names for pyarrow.
            include_columns = [f&#34;f{column_idx}&#34; for column_idx in projection]

        tbl = pa.csv.read_csv(
            file,
            pa.csv.ReadOptions(
                skip_rows=skip_rows, autogenerate_column_names=not has_headers
            ),
            pa.csv.ParseOptions(delimiter=sep),
            pa.csv.ConvertOptions(
                column_types=None,
                include_columns=include_columns,
                include_missing_columns=ignore_errors,
            ),
        )

        if new_columns:
            tbl = tbl.rename_columns(new_columns)
        elif not has_headers:
            # Rename &#39;f0&#39;, &#39;f1&#39;, ... columns names autogenated by pyarrow to &#39;column_1&#39;, &#39;column_2&#39;, ...
            tbl = tbl.rename_columns(
                [f&#34;column_{int(column[1:]) + 1}&#34; for column in tbl.column_names]
            )

        return from_arrow(tbl, rechunk)

    def read_csv_to_df(file):
        return DataFrame.read_csv(
            file=file,
            infer_schema_length=infer_schema_length,
            batch_size=batch_size,
            has_headers=has_headers,
            ignore_errors=ignore_errors,
            stop_after_n_rows=stop_after_n_rows,
            skip_rows=skip_rows,
            projection=projection,
            sep=sep,
            columns=columns,
            rechunk=rechunk,
            encoding=encoding,
            n_threads=n_threads,
            dtype=dtype,
            low_memory=low_memory,
        )

    if isinstance(file, str) and file.endswith(&#34;.gz&#34;):
        try:
            # Try to use python-isal for faster gzip decompresssion.
            from isal import igzip as gzip_mod
        except ImportError:
            # Use normal gzip module, if python-isal is not installed.
            import gzip as gzip_mod

        with gzip_mod.open(file, &#34;rb&#34;) as fh:
            df = read_csv_to_df(fh)
    else:
        df = read_csv_to_df(file)

    if new_columns:
        df.columns = new_columns
    return df</code></pre>
</details>
</dd>
<dt id="polars.functions.read_ipc"><code class="name flex">
<span>def <span class="ident">read_ipc</span></span>(<span>file: Union[str, BinaryIO, pathlib.Path], use_pyarrow: bool = True) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Read into a DataFrame from Arrow IPC stream format. This is also called the feather format.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>Path to a file or a file like object.</dd>
<dt><strong><code>use_pyarrow</code></strong></dt>
<dd>Use pyarrow or rust arrow backend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_ipc(file: Union[str, BinaryIO, Path], use_pyarrow: bool = True) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Read into a DataFrame from Arrow IPC stream format. This is also called the feather format.

    Parameters
    ----------
    file
        Path to a file or a file like object.
    use_pyarrow
        Use pyarrow or rust arrow backend.

    Returns
    -------
    DataFrame
    &#34;&#34;&#34;
    file = _prepare_file_arg(file)
    return DataFrame.read_ipc(file, use_pyarrow)</code></pre>
</details>
</dd>
<dt id="polars.functions.read_json"><code class="name flex">
<span>def <span class="ident">read_json</span></span>(<span>source: Union[str, StringIO, Path]) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Read into a DataFrame from JSON format.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>source</code></strong></dt>
<dd>Path to a file or a file like object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_json(
    source: &#34;Union[str, StringIO, Path]&#34;,
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Read into a DataFrame from JSON format.

    Parameters
    ----------
    source
        Path to a file or a file like object.
    &#34;&#34;&#34;
    return DataFrame.read_json(source)</code></pre>
</details>
</dd>
<dt id="polars.functions.read_parquet"><code class="name flex">
<span>def <span class="ident">read_parquet</span></span>(<span>source: Union[str, BinaryIO, Path, List[str]], stop_after_n_rows: Optional[int] = None, memory_map=True, columns: Union[List[str], NoneType] = None, **kwargs) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Read into a DataFrame from a parquet file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>source</code></strong></dt>
<dd>Path to a file | list of files, or a file like object. If the path is a directory, that directory will be used
as partition aware scan.</dd>
<dt><strong><code>stop_after_n_rows</code></strong></dt>
<dd>After n rows are read from the parquet, it stops reading. Note: this cannot be used in partition aware parquet
reads.</dd>
<dt><strong><code>memory_map</code></strong></dt>
<dd>Memory map underlying file. This will likely increase performance.</dd>
<dt><strong><code>columns</code></strong></dt>
<dd>Columns to project/ select.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>kwargs for <a href="https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html">pyarrow.parquet.read_table</a></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_parquet(
    source: &#34;Union[str, BinaryIO, Path, List[str]]&#34;,
    stop_after_n_rows: &#34;Optional[int]&#34; = None,
    memory_map=True,
    columns: Optional[List[str]] = None,
    **kwargs,
) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    Read into a DataFrame from a parquet file.

    Parameters
    ----------
    source
        Path to a file | list of files, or a file like object. If the path is a directory, that directory will be used
        as partition aware scan.
    stop_after_n_rows
        After n rows are read from the parquet, it stops reading. Note: this cannot be used in partition aware parquet
        reads.
    memory_map
        Memory map underlying file. This will likely increase performance.
    columns
        Columns to project/ select.
    **kwargs
        kwargs for [pyarrow.parquet.read_table](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)

    Returns
    -------
    DataFrame
    &#34;&#34;&#34;
    source = _prepare_file_arg(source)
    if stop_after_n_rows is not None:
        return DataFrame.read_parquet(source, stop_after_n_rows=stop_after_n_rows)
    else:
        return from_arrow(
            pa.parquet.read_table(
                source, memory_map=memory_map, columns=columns, **kwargs
            )
        )</code></pre>
</details>
</dd>
<dt id="polars.functions.read_sql"><code class="name flex">
<span>def <span class="ident">read_sql</span></span>(<span>sql: str, engine) ‑> <a title="polars.frame.DataFrame" href="frame.html#polars.frame.DataFrame">DataFrame</a></span>
</code></dt>
<dd>
<div class="desc"><h1 id="preface">Preface</h1>
<p>Deprecated by design. Will not have a long future support and no guarantees given whatsoever.
Want backwards compatibility?</p>
<p>Use:</p>
<pre><code class="language-python">df = pl.from_pandas(pd.read_sql(sql, engine))
</code></pre>
<p>The support is limited because I want something better.</p>
<h1 id="docstring">Docstring</h1>
<p>Load a DataFrame from a database by sending a raw sql query.
Make sure to install sqlalchemy ^1.4</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sql</code></strong></dt>
<dd>raw sql query</dd>
<dt><strong><code>engine</code></strong> :&ensp;<code>sqlalchemy engine</code></dt>
<dd>make sure to install sqlalchemy ^1.4</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_sql(sql: str, engine) -&gt; &#34;DataFrame&#34;:
    &#34;&#34;&#34;
    # Preface
    Deprecated by design. Will not have a long future support and no guarantees given whatsoever.
    Want backwards compatibility?

    Use:

    ```python
    df = pl.from_pandas(pd.read_sql(sql, engine))
    ```

    The support is limited because I want something better.

    # Docstring
    Load a DataFrame from a database by sending a raw sql query.
    Make sure to install sqlalchemy ^1.4

    Parameters
    ----------
    sql
        raw sql query
    engine : sqlalchemy engine
        make sure to install sqlalchemy ^1.4
    &#34;&#34;&#34;
    try:
        # pandas sql loading is faster.
        # conversion from pandas to arrow is very cheap compared to db driver
        import pandas as pd

        return from_pandas(pd.read_sql(sql, engine))
    except ImportError:
        from sqlalchemy import text

        with engine.connect() as con:
            result = con.execute(text(sql))

        rows = result.fetchall()
        return from_rows(rows, list(result.keys()))</code></pre>
</details>
</dd>
<dt id="polars.functions.repeat"><code class="name flex">
<span>def <span class="ident">repeat</span></span>(<span>val: Union[int, float, str], n: int, name: Union[str, NoneType] = None) ‑> <a title="polars.series.Series" href="series.html#polars.series.Series">Series</a></span>
</code></dt>
<dd>
<div class="desc"><p>Repeat a single value n times and collect into a Series.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>val</code></strong></dt>
<dd>Value to repeat.</dd>
<dt><strong><code>n</code></strong></dt>
<dd>Number of repeats.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>Optional name of the Series.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def repeat(
    val: &#34;Union[int, float, str]&#34;, n: int, name: Optional[str] = None
) -&gt; &#34;Series&#34;:
    &#34;&#34;&#34;
    Repeat a single value n times and collect into a Series.

    Parameters
    ----------
    val
        Value to repeat.
    n
        Number of repeats.
    name
        Optional name of the Series.
    &#34;&#34;&#34;
    if name is None:
        name = &#34;&#34;
    if isinstance(val, str):
        s = Series._repeat(name, val, n)
        s.rename(name)
        return s
    else:
        return Series.from_arrow(name, pa.repeat(val, n))</code></pre>
</details>
</dd>
<dt id="polars.functions.scan_csv"><code class="name flex">
<span>def <span class="ident">scan_csv</span></span>(<span>file: Union[str, pathlib.Path], has_headers: bool = True, ignore_errors: bool = False, sep: str = ',', skip_rows: int = 0, stop_after_n_rows: Optional[int] = None, cache: bool = True, dtype: Optional[Dict[str, DataType]] = None, low_memory: bool = False) ‑> <a title="polars.lazy.LazyFrame" href="lazy/index.html#polars.lazy.LazyFrame">LazyFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Lazily read from a csv file.</p>
<p>This allows the query optimizer to push down predicates and projections to the scan level,
thereby potentially reducing memory overhead.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>Path to a file.</dd>
<dt><strong><code>has_headers</code></strong></dt>
<dd>If the CSV file has headers or not.</dd>
<dt><strong><code>ignore_errors</code></strong></dt>
<dd>Try to keep reading lines if some lines yield errors.</dd>
<dt><strong><code>sep</code></strong></dt>
<dd>Delimiter/ value separator.</dd>
<dt><strong><code>skip_rows</code></strong></dt>
<dd>Start reading after <code>skip_rows</code>.</dd>
<dt><strong><code>stop_after_n_rows</code></strong></dt>
<dd>After n rows are read from the CSV, it stops reading.
During multi-threaded parsing, an upper bound of <code>n</code> rows
cannot be guaranteed.</dd>
<dt><strong><code>cache</code></strong></dt>
<dd>Cache the result after reading.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Overwrite the dtypes during inference.</dd>
<dt><strong><code>low_memory</code></strong></dt>
<dd>Reduce memory usage in expense of performance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scan_csv(
    file: Union[str, Path],
    has_headers: bool = True,
    ignore_errors: bool = False,
    sep: str = &#34;,&#34;,
    skip_rows: int = 0,
    stop_after_n_rows: &#34;Optional[int]&#34; = None,
    cache: bool = True,
    dtype: &#34;Optional[Dict[str, DataType]]&#34; = None,
    low_memory: bool = False,
) -&gt; &#34;LazyFrame&#34;:
    &#34;&#34;&#34;
    Lazily read from a csv file.

    This allows the query optimizer to push down predicates and projections to the scan level,
    thereby potentially reducing memory overhead.

    Parameters
    ----------
    file
        Path to a file.
    has_headers
        If the CSV file has headers or not.
    ignore_errors
        Try to keep reading lines if some lines yield errors.
    sep
        Delimiter/ value separator.
    skip_rows
        Start reading after `skip_rows`.
    stop_after_n_rows
        After n rows are read from the CSV, it stops reading.
        During multi-threaded parsing, an upper bound of `n` rows
        cannot be guaranteed.
    cache
        Cache the result after reading.
    dtype
        Overwrite the dtypes during inference.
    low_memory
        Reduce memory usage in expense of performance.
    &#34;&#34;&#34;
    if isinstance(file, Path):
        file = str(file)
    return LazyFrame.scan_csv(
        file=file,
        has_headers=has_headers,
        sep=sep,
        ignore_errors=ignore_errors,
        skip_rows=skip_rows,
        stop_after_n_rows=stop_after_n_rows,
        cache=cache,
        dtype=dtype,
        low_memory=low_memory,
    )</code></pre>
</details>
</dd>
<dt id="polars.functions.scan_parquet"><code class="name flex">
<span>def <span class="ident">scan_parquet</span></span>(<span>file: Union[str, pathlib.Path], stop_after_n_rows: Optional[int] = None, cache: bool = True) ‑> <a title="polars.lazy.LazyFrame" href="lazy/index.html#polars.lazy.LazyFrame">LazyFrame</a></span>
</code></dt>
<dd>
<div class="desc"><p>Lazily read from a parquet file.</p>
<p>This allows the query optimizer to push down predicates and projections to the scan level,
thereby potentially reducing memory overhead.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>Path to a file.</dd>
<dt><strong><code>stop_after_n_rows</code></strong></dt>
<dd>After n rows are read from the parquet, it stops reading.</dd>
<dt><strong><code>cache</code></strong></dt>
<dd>Cache the result after reading.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scan_parquet(
    file: Union[str, Path],
    stop_after_n_rows: &#34;Optional[int]&#34; = None,
    cache: bool = True,
) -&gt; &#34;LazyFrame&#34;:
    &#34;&#34;&#34;
    Lazily read from a parquet file.

    This allows the query optimizer to push down predicates and projections to the scan level,
    thereby potentially reducing memory overhead.

    Parameters
    ----------
    file
        Path to a file.
    stop_after_n_rows
        After n rows are read from the parquet, it stops reading.
    cache
        Cache the result after reading.
    &#34;&#34;&#34;
    if isinstance(file, Path):
        file = str(file)
    return LazyFrame.scan_parquet(
        file=file, stop_after_n_rows=stop_after_n_rows, cache=cache
    )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<a class="homelink" rel="home" title="Home" href="/polars/python/polars/index.html">
<img src="/polars/img/polars_logo.png" alt=""> polars </a>
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="polars" href="index.html">polars</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="polars.functions.arg_where" href="#polars.functions.arg_where">arg_where</a></code></li>
<li><code><a title="polars.functions.concat" href="#polars.functions.concat">concat</a></code></li>
<li><code><a title="polars.functions.from_arrow" href="#polars.functions.from_arrow">from_arrow</a></code></li>
<li><code><a title="polars.functions.from_arrow_table" href="#polars.functions.from_arrow_table">from_arrow_table</a></code></li>
<li><code><a title="polars.functions.from_pandas" href="#polars.functions.from_pandas">from_pandas</a></code></li>
<li><code><a title="polars.functions.from_rows" href="#polars.functions.from_rows">from_rows</a></code></li>
<li><code><a title="polars.functions.get_dummies" href="#polars.functions.get_dummies">get_dummies</a></code></li>
<li><code><a title="polars.functions.read_csv" href="#polars.functions.read_csv">read_csv</a></code></li>
<li><code><a title="polars.functions.read_ipc" href="#polars.functions.read_ipc">read_ipc</a></code></li>
<li><code><a title="polars.functions.read_json" href="#polars.functions.read_json">read_json</a></code></li>
<li><code><a title="polars.functions.read_parquet" href="#polars.functions.read_parquet">read_parquet</a></code></li>
<li><code><a title="polars.functions.read_sql" href="#polars.functions.read_sql">read_sql</a></code></li>
<li><code><a title="polars.functions.repeat" href="#polars.functions.repeat">repeat</a></code></li>
<li><code><a title="polars.functions.scan_csv" href="#polars.functions.scan_csv">scan_csv</a></code></li>
<li><code><a title="polars.functions.scan_parquet" href="#polars.functions.scan_parquet">scan_parquet</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>© Copyright 2021, polars development team</p>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>